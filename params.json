{"name":"Trex","tagline":"A multi-Paxos transaction log replication engine","body":"## Building\r\n\r\n```\r\n# kick the tires\r\nsbt clean coverage test it:test\r\nsbt coverageReport\r\n```\r\n[![Build Status](https://travis-ci.org/simbo1905/trex.svg?branch=master)](https://travis-ci.org/simbo1905/trex)\r\n\r\n## What is this? \r\n\r\nThis is TRex which has a [write up here](https://simbo1905.wordpress.com/2014/10/28/transaction-log-replication-with-paxos/). \r\n\r\nMore to come...\r\n\r\n## How do I use this? \r\n\r\nBelow is a rough sketch of an article to introduce the demo app. \r\n\r\n## Attribution\r\n\r\nThe TRex icon is Tyrannosaurus Rex by Raf Verbraeken from the Noun Project licensed under [CC3.0](http://creativecommons.org/licenses/by/3.0/us/)\r\n\r\n### TRex The Paxos Engine\r\n\r\nPrevious [posts](https://simbo1905.wordpress.com/2014/10/28/transaction-log-replication-with-paxos/) described how the Paxos algorithm can be applied to binary log replication. This post introduces a Paxos state machine replication engine called TRex. TRex is implemented in Scala using Akka FSM. It can be used to reliably replicate messages, commands or method invocations to create a strongly consistent and fault tolerate cluster with automatic leader failover. The demo code for TRex shows how to wrap an object so that it is replicated across a Paxos cluster. You can fork the code over at TBD.\r\n\r\nThis post will provide a walk-through of the demo application. A detailed description of how multi-Paxos is implemented in TRex is given in a previous post. TRex also comes with extensive unit tests that are an executable specification of its implementation of the multi-Paxos consensus algorithm. \r\n\r\nBefore we begin I should make a statement about the status of the codebase. Currently this post describes version 0.5.0 of the code. Some features that a production replication service would need to provide (e.g. metrics, dynamic cluster membership) are not yet implemented. This blog post will be updated as new features are added to TRex. \r\n\r\nThe demo application implements a key-value store that has the following interface:\r\n\r\n```Scala\r\ntrait ConsistentKVStore {\r\n  /**\r\n   * Add a value into the KV store\r\n   */\r\n  def put(key: String, value: String): Unit\r\n\r\n  /**\r\n   * Add a value into the KV store only if the current version number is 'version'\r\n   * @return True if the operation succeeded else False\r\n   */\r\n  def put(key: String, value: String, version: Long): Boolean\r\n\r\n  /**\r\n   * Remove a value form the store.\r\n   */\r\n  def remove(key: String): Unit\r\n\r\n  /**\r\n   * Remove a value from the KV store only if the current version number is 'version'\r\n   * @return\r\n   */\r\n  def remove(key: String, version: Long): Boolean\r\n\r\n  /**\r\n   * Read a value and its version number from the KV store.\r\n   * The setting of the ‘consistent’ flag is used\r\n   * to choose the consistency level. Setting it to ‘true’\r\n   * chooses strong consistency, and the latest value is always\r\n   * returned. Setting it to ‘false’ chooses timeline\r\n   * consistency, and a possibly stale value is returned in\r\n   * exchange for better performance.\r\n   * @param key The key of the value to get\r\n   * @return A tuple of the value and the version number of the value of the key\r\n   */\r\n  def get(key: String): Option[(String,Long)]\r\n}\r\n``` \r\n\r\nThe actual implementation isn't important for the purposes of the demo. It simply represents an application service that we will replicate to achieve fault tolerance. We should note that the write methods are safe to repeat during crash recovery. TRex journals that it has completed chosen commands immediately after they have been run. If a crash happens before the journal is flushed then a command will be rerun after the node is restarted. If your application API is not \"recovery replay safe\" you should code a custom journal class which participates in your application transactions. You would also need to override the TRex Paxos message sending methods to buffer outbound messages. Either send the buffered massages post-commit else drop them post-rollback. \r\n\r\nNow that we understand the object we wish to replicate we can go step-by-step through [the demo code]. It starts with a static TRex cluster configuration:\r\n\r\n```\r\n# trex simple cluster configuration\r\ntrex {\r\n  # folder to use to persist data at each node\r\n  data-folder=\"/tmp\"\r\n  # number of slots entries to retain in the log to support retransmission\r\n  data-retained=1048576\r\n  # static cluster definintion\r\n  cluster {\r\n    name = \"PaxosKVStore\"\r\n    nodes = \"2552,2562,2572\"\r\n    node-2552 {\r\n      host = \"127.0.0.1\"\r\n      client-port = 2552\r\n      node-port = 2553\r\n    }\r\n    node-2562 {\r\n      host = \"127.0.0.1\"\r\n      client-port = 2562\r\n      node-port = 2563\r\n    }\r\n    node-2572 {\r\n      host = \"127.0.0.1\"\r\n      client-port = 2572\r\n      node-port = 2573\r\n    }\r\n  }\r\n  # timeouts\r\n  leader-timeout-max=4000\r\n  leader-timeout-min=2000\r\n}\r\n```\r\n\r\nThat defines a TRex cluster with three server nodes on localhost exposing a TCP port to clients and a UDP port to the other cluster nodes. A more typical deployment would use three separate hosts and the same ports on each. The client application loads the configuration then creates a dynamic proxy backed by the Paxos cluster:\r\n\r\n```Scala\r\n    val cluster = Cluster.parseConfig(config)\r\n\r\n    val system =\r\n      ActorSystem(cluster.name, ConfigFactory.load(\"client.conf\"))\r\n\r\n    val timeout = Timeout(100 millisecond)\r\n\r\n    val driver = system.actorOf(Props(classOf[StaticClusterDriver], timeout, cluster, 20), \"TrexDriver\")\r\n\r\n    val typedActor: ConsistentKVStore =\r\n      TypedActor(system).\r\n        typedActorOf(\r\n          TypedProps[ConsistentKVStore],\r\n          driver)\r\n```\r\n \r\nThat code creates a TRex client driver passing in the cluster configuration. It then creates a TypedActor proxy which forwards onto the TRex driver. The TypedActor proxy pattern is an out-of-the-box Akka feature. It gives us an implementation of the application interface which serialises every method invocation as a MethodCall message. This is sent to the TRex driver which dispatches the message to the current Paxos distinguished leader. We should note that the TRex driver it is agnostic to the client-to-server application protocol. If your application is already using Akka you could just send your existing messages to the driver. You can even configure Akka with a custom serialiser to control the wire format. \r\n\r\nThat's it. The remainder of the client application is a trivial loop reading input commands to invoke methods on the interface. The same loop would work invoking methods directly on a local object. We can think of the few lines of Akka code shown above as a drop-in replacement for a local object. The drop-in code creates a stub of the application which is a dynamic proxy backed by a cluster of replicated objects with automatic failover. Next we need to setup the server cluster. \r\n\r\nThe server code is as follows:  \r\n\r\n```Scala\r\n      // the client app K-V store\r\n      val dataFile = new java.io.File(folder.getCanonicalPath + \"/kvstore\")\r\n      println(s\"node kv data store is ${dataFile.getCanonicalPath}\")\r\n      val db: DB = DBMaker.newFileDB(dataFile).make\r\n      val clientApp = new MapDBConsistentKVStore(db)\r\n      val logFile = new java.io.File(folder.getCanonicalPath + \"/paxos\")\r\n      println(s\"paxos data log is ${logFile.getCanonicalPath}\")\r\n      val journal = new FileJournal(logFile, cluster.retained)\r\n      // the node unique id in the paxos closter which is passed into main\r\n      val node = nodeMap.get(nodeId).get\r\n      // actor system with the node config\r\n      val system =\r\n        ActorSystem(cluster.name, ConfigFactory.load(\"server.conf\").withValue(\"akka.remote.netty.tcp.port\",ConfigValueFactory.fromAnyRef(node.clientPort) ))\r\n      // generic entry point accepts TypedActor MethodCall messages and reflectively invokes them on our client app\r\n      system.actorOf(Props(classOf[TypedActorPaxosEndpoint], cluster, PaxosActor.Configuration(config, cluster.nodes.size), node.id, journal, clientApp, \"TrexServer\"))\r\n```\r\n\r\nThere is quite a bit going on there due to the separation of concerns in the code. The application object `val clientApp = new MapDBConsistentKVStore(db))` has its own concerns and has no dependencies on TRex. The journal object encapsulates the concern of journaling the progress of the consensus algorithm. As noted above an application which is not crash recovery replay safe would need to provide a custom journal that participates in application transactions. The `TypedActorPaxosEndpoint` is our Trex Server which runs the consensus algorithm.\r\n\r\nThe most interesting actor for the purpose of a demo of integrating TRex into an existing application is the `TypedActorPaxosEndpoint`. This runs the consensus algorithm over values sent from the TRex driver by clients. In this case the values are `MethodCall` messages sent from the `TypedActor` at the client. This endpoint reflectively invokes the select commands on its local copy of the replicated application object. The consensus logic is in a superclass so you can write a custom endpoint actor by overriding the `deliver` method. You can then compile the deliver logic directly against your own application code and message formats. \r\n\r\nNow we have both client and server its worth describing the full data flow created by the demo code. The client code invokes the interface methods on the `TypedActor`. The `TypedActor` forwards a `MethodCall` message to the TRex driver. The driver has Akka serialise the message and forwards it to the distinguished leader over TCP. At the other end the distinguished leader runs the multi-Paxos consensus algorithm over UDP. The chosen messages are invoked on all the replicated objects in consensus order. The method return value at the distinguished leader is sent back to the client TRex driver. It responds to the TypedActor which competes the method call for the client code. \r\n\r\nWe can now start up the three server process and multiple clients. We can run commands though the clients and kill the leader node to see a failover. If we restart the failed node it will automatically sync up with the new leader. Not bad for a few lines of custom code and a bit of configuration. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}